// console.log(2<1);
// console.log(2>1);  //true
// console.log(2<1);
// console.log(2<=1);
// console.log(2>=1);  // true
// console.log(+false);  //0
// console.log(true); // true


// console.log("2">1);
// console.log("2"<1);
// console.log("2"==1);
// console.log("2">=1);
// console.log("2"<=1);

// console.log("hello"=="olleh");  // false
// "5" <= 3; // false
// "3" <= 3; // true
// "3" <= 5; // true

// "hello" <= 5; // false
// 5 <= "hello"; // false


// true <= false; // false
// true <= true; // true
// false <= true; // true

// true <= 0; // false
// true <= 1; // true

// console.log(null >0);  //false
// console.log(null == 0);  // false
// console.log(null < 0); // false
// console.log(null <= 0);  //true
// console.log(null >= 0);  // true

/*  Because and equality check == and comparisons >< >= <= works differently.
Comparisons convert null to a number , treating it as 0.

That's why null>=0 and null<=0 gives true and null>0 and null<0 as false
*/

// null <= 0; // true
// 1 <= null; // false

// undefined <= 3; // false
// 3 <= undefined; // false

// 3 <= NaN; // false
// NaN <= 3; // false


// console.log(undefined ==0); //false
// console.log(NaN ==0);  // false

// console.log(undefined == NaN);
// console.log(undefined > NaN);   false
// console.log(undefined < NaN);



// === (check strictly including datatype)

// console.log("2"===2);  // false
// console.log("2"==2);  // true





